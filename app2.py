import streamlit as st
import feedparser
import pandas as pd
import numpy as np
import yfinance as yf
from textblob import TextBlob
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import time
import urllib.parse
import os # å¼•å…¥ OS æ¨¡çµ„é€²è¡Œæœ¬æ©Ÿæª”æ¡ˆæ“ä½œ
import plotly.graph_objects as go
from sklearn.ensemble import RandomForestRegressor

# ==========================================
# 0. é é¢è¨­å®šèˆ‡æœ¬æ©Ÿæª”æ¡ˆæª¢æŸ¥
# ==========================================
st.set_page_config(page_title="App 11.1 åè„†å¼±æŒ‡æ®å®˜", layout="wide")

LOCAL_NEWS_FILE = "news_data_local.csv"

# åˆå§‹åŒ–ï¼šå¦‚æœæœ¬æ©Ÿæœ‰æª”æ¡ˆï¼Œç›´æ¥è¼‰å…¥
if 'news_data' not in st.session_state:
    if os.path.exists(LOCAL_NEWS_FILE):
        try:
            df_local = pd.read_csv(LOCAL_NEWS_FILE)
            if 'Date' in df_local.columns:
                df_local['Date'] = pd.to_datetime(df_local['Date'])
            st.session_state['news_data'] = df_local
            st.toast(f"âœ… å·²è‡ªå‹•è¼‰å…¥æœ¬æ©Ÿå­˜æª”ï¼š{len(df_local)} ç­†æ–°è", icon="ğŸ“‚")
        except:
            st.session_state['news_data'] = pd.DataFrame()
    else:
        st.session_state['news_data'] = pd.DataFrame()

st.title("ğŸ¦… App 11.1: åè„†å¼±æŒ‡æ®å®˜ (æœ¬æ©Ÿå­˜æª”å¢å¼·ç‰ˆ)")
st.markdown("""
**æ ¸å¿ƒå‡ç´šï¼š**
1.  **æœ¬æ©ŸæŒä¹…åŒ–**ï¼šæ–°èæŠ“å–å¾Œç›´æ¥å¯«å…¥ç¡¬ç¢Ÿ `news_data_local.csv`ï¼Œé‡æ•´ç¶²é è³‡æ–™ä¸éºå¤±ã€‚
2.  **è‡ªå‹•è¼‰å…¥**ï¼šç¨‹å¼å•Ÿå‹•æ™‚æœƒå„ªå…ˆè®€å–æœ¬æ©ŸèˆŠæª”ï¼Œç¯€çœçˆ¬èŸ²æ™‚é–“ã€‚
""")

# ==========================================
# 1. æ–°èçˆ¬èŸ² (å«å¯«å…¥ç¡¬ç¢ŸåŠŸèƒ½)
# ==========================================
TICKER_MAP = {
    'TSM': {'TW': 'å°ç©é›»', 'JP': 'TSMC', 'EU': 'TSMC'},
    'NVDA': {'TW': 'è¼é”', 'JP': 'NVIDIA', 'EU': 'Nvidia'},
    'AMD': {'TW': 'è¶…å¾®', 'JP': 'AMD', 'EU': 'AMD'},
    'URA': {'TW': 'éˆ¾ç¤¦', 'JP': 'ã‚¦ãƒ©ãƒ³', 'EU': 'Uranium'},
    'SOXL': {'TW': 'åŠå°é«”', 'JP': 'åŠå°ä½“', 'EU': 'Semiconductor'},
    'BTC-USD': {'TW': 'æ¯”ç‰¹å¹£', 'JP': 'ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³', 'EU': 'Bitcoin'}
}

MULTILINGUAL_DICT = {
    'ZH': {'UP': ['å¤§æ¼²','æ¼²åœ','å‰µé«˜','åˆ©å¤š','çˆ†ç™¼','æ“´ç”¢','æ€¥å–®'], 'DOWN': ['å¤§è·Œ','è·Œåœ','é‡æŒ«','åˆ©ç©º','ç å–®','è¡°é€€']},
    'JA': {'UP': ['ä¸Šæ˜‡','æ€¥é¨°','æœ€é«˜å€¤','å¥½èª¿','å¢—ç›Š'], 'DOWN': ['ä¸‹è½','æ€¥è½','æœ€å®‰å€¤','ä¸èª¿','æ¸›ç›Š']},
    'DE': {'UP': ['anstieg','rekord','gewinn','kaufen'], 'DOWN': ['verlust','fallen','krise','verkaufen']}
}

def fetch_global_news_12m(ticker):
    news_history = []
    end_date = datetime.now()
    start_date = end_date - relativedelta(months=12) 
    map_info = TICKER_MAP.get(ticker, {})
    term_us = f"{ticker}+stock" if len(ticker) <= 4 else ticker
    
    term_tw = urllib.parse.quote(map_info.get('TW', ticker))
    term_jp = urllib.parse.quote(map_info.get('JP', ticker))
    term_eu = urllib.parse.quote(map_info.get('EU', ticker))

    current = start_date
    while current < end_date:
        next_month = current + relativedelta(months=1)
        d_after = current.strftime('%Y-%m-%d')
        d_before = next_month.strftime('%Y-%m-%d')
        
        urls = [
            (f"https://news.google.com/rss/search?q={term_us}+after:{d_after}+before:{d_before}&hl=en-US&gl=US&ceid=US:en", 'US'),
            (f"https://news.google.com/rss/search?q={term_us}+after:{d_after}+before:{d_before}&hl=en-GB&gl=GB&ceid=GB:en", 'EU_UK')
        ]
        if ticker in ['TSM', 'NVDA', 'AMD', '0050.TW', 'CLS', 'SOXL']:
            urls.append((f"https://news.google.com/rss/search?q={term_tw}+after:{d_after}+before:{d_before}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant", 'TW'))
        if ticker in ['TSM', 'NVDA', 'SOXL', 'URA', 'BTC-USD']:
            urls.append((f"https://news.google.com/rss/search?q={term_jp}+after:{d_after}+before:{d_before}&hl=ja&gl=JP&ceid=JP:ja", 'JP'))
        if ticker in ['URA', 'SOXL', 'CLS', 'AMD']:
            urls.append((f"https://news.google.com/rss/search?q={term_eu}+after:{d_after}+before:{d_before}&hl=de&gl=DE&ceid=DE:de", 'EU_DE'))

        for url, region in urls:
            try:
                feed = feedparser.parse(url)
                for entry in feed.entries[:2]: 
                    title = entry.title
                    pub_date = pd.to_datetime(entry.published).date() if hasattr(entry, 'published') else current.date()
                    score = 0
                    if region in ['US', 'EU_UK']:
                        score = TextBlob(title).sentiment.polarity
                        if any(x in title.lower() for x in ['beat', 'surge', 'record']): score += 0.3
                    elif region == 'TW':
                        for k in MULTILINGUAL_DICT['ZH']['UP']: 
                            if k in title: score += 0.5
                    elif region == 'JP':
                        for k in MULTILINGUAL_DICT['JA']['UP']: 
                            if k in title: score += 0.5
                    elif region == 'EU_DE':
                        for k in MULTILINGUAL_DICT['DE']['UP']: 
                            if k in title.lower(): score += 0.5
                    if score != 0:
                        news_history.append({'Ticker': ticker, 'Date': pub_date, 'Region': region, 'Title': title, 'Score': score})
            except: pass
        current = next_month
        time.sleep(0.05)
    return pd.DataFrame(news_history)

# ==========================================
# 2. å®šåƒ¹å±¤ (å››ç¶­å®šåƒ¹)
# ==========================================
def train_rf_model(df, ticker):
    try:
        data = df[['Close']].copy()
        data['Ret'] = data['Close'].pct_change()
        data['Vol'] = data['Ret'].rolling(20).std()
        data['SMA'] = data['Close'].rolling(20).mean()
        data['Target'] = data['Close'].shift(-5) 
        data = data.dropna()
        if len(data) < 60: return None
        X = data[['Ret', 'Vol', 'SMA']]
        y = data['Target']
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X, y)
        last_row = data.iloc[[-1]][['Ret', 'Vol', 'SMA']]
        return model.predict(last_row)[0]
    except: return None

def calc_4d_target(ticker, df_price):
    current = df_price['Close'].iloc[-1]
    tr = df_price['High'] - df_price['Low']
    atr = tr.rolling(14).mean().iloc[-1]
    t_atr = current + (atr * np.sqrt(5))
    recent = df_price['Close'].iloc[-60:]
    t_fib = recent.max() + (recent.max() - recent.min()) * 0.618
    mu = df_price['Close'].pct_change().mean()
    t_mc = current * ((1 + mu) ** 5)
    t_rf = train_rf_model(df_price, ticker)
    if t_rf is None: t_rf = t_mc
    avg_target = (t_atr + t_fib + t_mc + t_rf) / 4
    return avg_target

# ==========================================
# 3. åè„†å¼±å›æ¸¬ (5æ—¥)
# ==========================================
def run_antifragile_backtest(df_price, df_news_ticker):
    df = df_price.copy()
    
    if not df_news_ticker.empty:
        if not pd.api.types.is_datetime64_any_dtype(df_news_ticker['Date']):
             df_news_ticker['Date'] = pd.to_datetime(df_news_ticker['Date'])
        df_news_ticker['Weight'] = df_news_ticker['Region'].apply(lambda x: 1.2 if x != 'US' else 1.0)
        df_news_ticker['W_Score'] = df_news_ticker['Score'] * df_news_ticker['Weight']
        daily_score = df_news_ticker.groupby('Date')['W_Score'].mean()
        df = df.join(daily_score, how='left').fillna(0)
        df['News_Roll'] = df['W_Score'].rolling(3).mean()
    else:
        df['News_Roll'] = 0
        
    df['MA20'] = df['Close'].rolling(20).mean()
    df['Bias'] = (df['Close'] - df['MA20']) / df['MA20']
    
    delta = df['Close'].diff()
    up = delta.clip(lower=0)
    down = -1 * delta.clip(upper=0)
    df['RSI'] = 100 - (100 / (1 + up.ewm(com=13).mean() / down.ewm(com=13).mean()))
    
    df['Ret_5D'] = df['Close'].shift(-5) / df['Close'] - 1
    
    # ç­–ç•¥ 1: é€†å‹¢æŠ„åº•
    buy_mask = (df['News_Roll'] < -0.1) & ((df['Bias'] < -0.03) | (df['RSI'] < 35))
    buy_opps = df[buy_mask].dropna(subset=['Ret_5D'])
    
    if len(buy_opps) > 0:
        win_rate = len(buy_opps[buy_opps['Ret_5D'] > 0]) / len(buy_opps)
        count = len(buy_opps)
        avg_ret = buy_opps['Ret_5D'].mean()
    else:
        win_rate = 0.0; count = 0; avg_ret = 0.0
        
    # ç­–ç•¥ 2: é †å‹¢è¿½é«˜ (å°ç…§çµ„)
    mom_mask = (df['News_Roll'] > 0.1) & (df['Bias'] > 0.03)
    mom_opps = df[mom_mask].dropna(subset=['Ret_5D'])
    if len(mom_opps) > 0:
        mom_win = len(mom_opps[mom_opps['Ret_5D'] > 0]) / len(mom_opps)
    else:
        mom_win = 0.0

    return win_rate, count, avg_ret, mom_win

# ==========================================
# 4. ä¸»ç¨‹å¼
# ==========================================
st.sidebar.title("æ§åˆ¶å°")
data_mode = st.sidebar.radio("æ•¸æ“šä¾†æº", ["1. å„ªå…ˆä½¿ç”¨æœ¬æ©Ÿ/è¨˜æ†¶é«”", "2. å¼·åˆ¶é‡æ–°æŠ“å– (Live)", "3. ä¸Šå‚³ CSV"])

default_tickers = ["TSM", "NVDA", "AMD", "SOXL", "URA", "CLS"]
user_tickers = st.sidebar.text_area("ä»£è™Ÿ", ", ".join(default_tickers))
ticker_list = [t.strip().upper() for t in user_tickers.split(',')]

# ç‹€æ…‹é¡¯ç¤º
if not st.session_state['news_data'].empty:
    st.sidebar.success(f"ç›®å‰è³‡æ–™åº«ï¼š{len(st.session_state['news_data'])} ç­†")
else:
    st.sidebar.warning("ç›®å‰è³‡æ–™åº«ç‚ºç©º")

# é‚è¼¯è™•ç†
if data_mode.startswith("2"): # å¼·åˆ¶é‡æŠ“
    if st.sidebar.button("ğŸš€ å•Ÿå‹•çˆ¬èŸ² (è¦†è“‹èˆŠæª”)"):
        all_news = []
        bar = st.sidebar.progress(0)
        for i, t in enumerate(ticker_list):
            df = fetch_global_news_12m(t)
            if not df.empty: all_news.append(df)
            bar.progress((i+1)/len(ticker_list))
            
        if all_news:
            news_df = pd.concat(all_news, ignore_index=True)
            # 1. å­˜å…¥ Session
            st.session_state['news_data'] = news_df
            # 2. å¯«å…¥æœ¬æ©Ÿç¡¬ç¢Ÿ (é—œéµï¼)
            news_df.to_csv(LOCAL_NEWS_FILE, index=False)
            st.sidebar.success(f"å·²æ›´æ–°ä¸¦å¯«å…¥ {LOCAL_NEWS_FILE}")
            
elif data_mode.startswith("3"): # ä¸Šå‚³
    up = st.sidebar.file_uploader("ä¸Šå‚³ news.csv", type=['csv'])
    if up:
        try:
            temp = pd.read_csv(up)
            temp['Date'] = pd.to_datetime(temp['Date'])
            st.session_state['news_data'] = temp
            # ä¹Ÿè¦å¯«å…¥æœ¬æ©Ÿï¼Œæ–¹ä¾¿ä¸‹æ¬¡ä½¿ç”¨
            temp.to_csv(LOCAL_NEWS_FILE, index=False) 
            st.sidebar.success("è®€å–ä¸¦å­˜æª”æˆåŠŸ")
        except: st.error("è®€æª”å¤±æ•—")

# åˆ†æåŸ·è¡Œ
if st.button("ğŸš€ åŸ·è¡Œåè„†å¼±åˆ†æ"):
    if st.session_state['news_data'].empty:
        st.error("è«‹å…ˆå–å¾—æ•¸æ“šï¼")
    else:
        st.subheader("ğŸ“Š åè„†å¼±æˆ°ç•¥å ±å‘Š (æœ¬æ©Ÿå­˜æª”ç‰ˆ)")
        news_df = st.session_state['news_data']
        results = []
        
        for t in ticker_list:
            df_price = yf.download(t, period="2y", progress=False, auto_adjust=True)
            if isinstance(df_price.columns, pd.MultiIndex):
                temp = df_price['Close'][[t]].copy(); temp.columns = ['Close']
                temp['High'] = df_price['High'][t]
                temp['Low'] = df_price['Low'][t]
                df_price = temp
            else:
                df_price = df_price[['Close', 'High', 'Low']]
            
            df_news_t = news_df[news_df['Ticker'] == t].copy()
            
            win_rate, count, avg_ret, mom_win = run_antifragile_backtest(df_price, df_news_t)
            target = calc_4d_target(t, df_price)
            
            current_close = df_price['Close'].iloc[-1]
            ma20 = df_price['Close'].rolling(20).mean().iloc[-1]
            bias = (current_close - ma20) / ma20
            
            latest_news_score = 0
            if not df_news_t.empty:
                df_news_t['Date'] = pd.to_datetime(df_news_t['Date'])
                last_news = df_news_t.sort_values('Date').iloc[-1]
                latest_news_score = last_news['Score']
            
            signal = "â¬œ è§€æœ›"
            if latest_news_score < -0.1 and bias < -0.03:
                signal = "ğŸ’ é€†å‹¢æŠ„åº•"
            elif latest_news_score > 0.3 and bias > 0.05:
                signal = "âš ï¸ éç†±è­¦æˆ’"

            results.append({
                'Ticker': t,
                'Anti_Win': win_rate,
                'Mom_Win': mom_win,
                'Count': count,
                'Avg_Ret_5D': avg_ret,
                'Current': current_close,
                'Target_5D': target,
                'Signal': signal,
                'News_Score': latest_news_score,
                'Bias': bias
            })
            
        res_df = pd.DataFrame(results)
        
        show = res_df.copy()
        show['Anti_Win'] = show['Anti_Win'].apply(lambda x: f"{x:.0%}")
        show['Mom_Win'] = show['Mom_Win'].apply(lambda x: f"{x:.0%}")
        show['Avg_Ret_5D'] = show['Avg_Ret_5D'].apply(lambda x: f"{x:+.1%}")
        show['Current'] = show['Current'].apply(lambda x: f"${x:.2f}")
        show['Target_5D'] = show['Target_5D'].apply(lambda x: f"${x:.2f}")
        show['News_Score'] = show['News_Score'].apply(lambda x: f"{x:.2f}")
        show['Bias'] = show['Bias'].apply(lambda x: f"{x:+.1%}")
        
        st.dataframe(show[['Ticker', 'Signal', 'Anti_Win', 'Mom_Win', 'Avg_Ret_5D', 'Current', 'Target_5D', 'News_Score', 'Bias']].style.map(
            lambda x: 'background-color: #00FF7F; color: black' if 'æŠ„åº•' in str(x) else '', subset=['Signal']
        ))